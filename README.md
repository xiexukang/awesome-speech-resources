# Awesome Speech Resources

ğŸ—£ï¸ A curated list of speech-related tools, papers, and datasets.  
*ç‚¹å‡»ä¸‹æ–¹åˆ†ç±»å±•å¼€è¯¦ç»†èµ„æºåˆ—è¡¨*

---

<details>
<summary><b>ğŸ”Š è¯­éŸ³è¯†åˆ« (ASR)</b></summary>

### å·¥å…·

| åç§° | æè¿° | é“¾æ¥ |
|------|------|------|
| Whisper | OpenAI å¼€æºçš„å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«æ¨¡å‹ | [GitHub](https://github.com/openai/whisper) |
| PaddleSpeech | ç™¾åº¦å¼€æºçš„è¯­éŸ³è¯†åˆ«å·¥å…·åŒ… | [GitHub](https://github.com/PaddlePaddle/PaddleSpeech) |
| awesome-whisper | whisper çš„ä¸€äº›åº”ç”¨å’Œä¼˜åŒ– | [GitHub](https://github.com/sindresorhus/awesome-whisper) |
| Kaldi | åŸºäºWFSTçš„ç»å…¸ASRå·¥å…·åŒ… | [å®˜ç½‘](https://kaldi-asr.org/) |
| ESPnet | ç«¯åˆ°ç«¯è¯­éŸ³å¤„ç†å·¥å…·åŒ… | [GitHub](https://github.com/espnet/espnet) |
| wenet | æ–°æ‰‹ä¸­æ–‡è¯­éŸ³è¯†åˆ«å·¥å…·åŒ… | [GitHub](https://github.com/wenet-e2e/wenet) |
| funasr | æ–°æ‰‹ä¸­æ–‡è¯­éŸ³è¯†åˆ«å·¥å…·åŒ… | [GitHub](https://github.com/modelscope/FunASR) |
| WeTextProcessor | è¯­éŸ³è¯†åˆ«TNå·¥å…·åŒ… | [GitHub](https://github.com/wenet-e2e/WeTextProcessing) |
| NeMo | Nvidiaç«¯åˆ°ç«¯è¯­éŸ³å¤„ç†å·¥å…·åŒ… | [GitHub](https://github.com/NVIDIA/NeMo) |
| Russian asr | ä¿„è¯­è¯†åˆ«æ¨ç†ä»£ç  | [GitHub](https://github.com/salute-developers/gigaam) |

### æ•°æ®é›†

| åç§° | æè¿° | é“¾æ¥ |
|------|------|------|
| LibriSpeech | è‹±æ–‡è¯­éŸ³æ•°æ®é›† | [å®˜ç½‘](http://www.openslr.org/12/) |
| AISHELL-1 | ä¸­æ–‡è¯­éŸ³æ•°æ®é›† | [å®˜ç½‘](http://www.openslr.org/33/) |
| AISHELL-3 | ä¸­æ–‡è¯­éŸ³æ•°æ®é›† | [å®˜ç½‘](http://www.openslr.org/68/) |
| voice_datasets |  open source voice and music datasets | [GitHub](https://github.com/jim-schwoebel/voice_datasets) |
| Common Voice | å¼€æºè¯­éŸ³æ•°æ®é›† | [å®˜ç½‘](https://commonvoice.mozilla.org/en) |
| Emilia | å¤šè¯­ç§å¼€æºè¯­éŸ³æ•°æ®é›† | [å®˜ç½‘](https://huggingface.co/datasets/amphion/Emilia-Dataset) |
| LibriVox | è‹±æ–‡è¯­éŸ³æ•°æ®é›† | [å®˜ç½‘](https://librivox.org/) |
| Mosel | æ¬§æ´²å¤šè¯­è¨€è¯­éŸ³æ•°æ®é›† | [huggingface](https://huggingface.co/datasets/FBK-MT/mosel) |
| ReazonSpeech | æ—¥èªè¯­éŸ³æ•°æ®é›† | [huggingface](https://huggingface.co/datasets/reazon-research/reazonspeech) |

### è®ºæ–‡

- **[Attention Is All You Need (2017)]**  
  Transformer æ¶æ„å¥ åŸºæ€§è®ºæ–‡ [[arXiv](https://arxiv.org/abs/1706.03762)]
- **[AST: Audio Spectrogram Transformer (2021)]**  
  AST [[arXiv](https://arxiv.org/pdf/2104.01778)]
- **[Whisper (2022)]**  
  å¤§è§„æ¨¡å¼±ç›‘ç£è¯­éŸ³è¯†åˆ« [[arXiv](https://arxiv.org/abs/2212.04356)]
- **[PSEUDO-LABELING FOR MASSIVELY MULTILINGUAL SPEECH RECOGNITION (2022)]**  
  å¤§è§„æ¨¡å¤šè¯­è¨€è¯­éŸ³è¯†åˆ« [[arXiv](https://arxiv.org/abs/2209.03143)]
- **[WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing (2022)]**  
  WavLM å¤§è§„æ¨¡è‡ªç›‘ç£é¢„è®­ç»ƒ [[arXiv](https://arxiv.org/pdf/2111.00161)]
- **[ZIPFORMER: A FASTER AND BETTER ENCODER FOR AUTOMATIC SPEECH RECOGNITION (2023)]**  
  ä¸€ç§æ›´å¿«æ›´å¥½çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç¼–ç å™¨ [[arXiv](https://arxiv.org/pdf/2310.11230)]
- **[FAST CONFORMER WITH LINEARLY SCALABLE ATTENTION FOR EFFICIENT SPEECH RECOGNITION (2023)]**  
  Fastconformerä¸€ç§ç”¨äºé«˜æ•ˆè¯­éŸ³è¯†åˆ« [[arXiv](https://arxiv.org/pdf/2305.05084)]
- **[An Embarrassingly Simple Approach for LLM with Strong ASR Capacity (2024)]**  
  LLM å¼ºå¤§çš„è¯­éŸ³è¯†åˆ«èƒ½åŠ› [[arXiv](https://arxiv.org/abs/2402.08846)]
- **[: 950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages (2024)]**
  æ¬§ç›Ÿè¯­è¨€çš„å¼€æ”¾å¼è¯­éŸ³åŸºç¡€æ¨¡å‹è®­ç»ƒçš„ 950,000 å°æ—¶è¯­éŸ³æ•°æ® [[arXiv](https://arxiv.org/pdf/2410.01036)]
- **[Retrieval Augmented Correction of Named Entity Speech Recognition Errors (2024)]**  
  æ£€ç´¢å¢å¼ºçš„å‘½åå®ä½“è¯­éŸ³è¯†åˆ«é”™è¯¯æ ¡æ­£ [[arXiv](https://arxiv.org/pdf/2409.06062)]
- **[Dynamic Language Group-based MoE: Enhancing Code-Switching Speech Recognition with Hierarchical Routing (2024)]**  
  åŸºäºåŠ¨æ€è¯­è¨€ç»„çš„åˆ†å±‚è·¯ç”± MoEï¼šå¢å¼ºCSè¯­éŸ³è¯†åˆ« [[arXiv](https://arxiv.org/pdf/2407.18581)]
- **[Efficient Streaming LLM for Speech Recognition (2025)]**  
  Metaæµå¼LLM è¯­éŸ³è¯†åˆ« [[arXiv](https://arxiv.org/abs/2410.03752)]
- **[Adapting Whisper for Streaming Speech Recognition via Two-Pass Decoding (2025)]**  
  U2 whisper æµå¼è¯­éŸ³è¯†åˆ« [[arXiv](https://www.arxiv.org/abs/2506.12154)]
- **[Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla (2025)]**  
  ä½èµ„æºè¯­è¨€ ASR æ¨¡å‹é€‚åº”æ€§ç ”ç©¶ï¼šWhisper ä¸ Wav2Vec-BERT åœ¨å­ŸåŠ æ‹‰è¯­ä¸Šçš„å¯¹æ¯”åˆ†æ [[arXiv](https://arxiv.org/pdf/2507.01931)]
- **[What do self-supervised speech models know about Dutch? Analyzing advantages of language-specific pre-training (2025)]**  
  è‡ªç›‘ç£è¯­éŸ³æ¨¡å‹å¯¹è·å…°è¯­è¯­è¨€ä¿¡æ¯çš„ç¼–ç èƒ½åŠ›ï¼Œä»¥åŠè¯­è¨€ç‰¹å®šé¢„è®­ç»ƒçš„ä¼˜åŠ¿ [[arXiv](https://arxiv.org/pdf/2506.00981v1.pdf)]
- **[BUT System for the MLC-SLM Challenge (2025)]**  
  2025å¹´Interspeech MLC-SLM æŒ‘æˆ˜èµ›è®ºæ–‡ [[arXiv](https://arxiv.org/pdf/2506.13414v1.pdf)]
- **[Bi-directional Context-Enhanced Speech Large Language Models for Multilingual Conversational ASR (2025)]**  
  Alibaba-NTU 2025å¹´Interspeech MLC-SLM æŒ‘æˆ˜èµ›è®ºæ–‡ [[arXiv](https://arxiv.org/pdf/2506.13396v1.pdf)]
- **[NTU Speechlab LLM-Based Multilingual ASR System for Interspeech MLC-SLM Challenge 2025]**  
  NTU 2025å¹´Interspeech MLC-SLM æŒ‘æˆ˜èµ›è®ºæ–‡ [[arXiv](https://arxiv.org/pdf/2506.13339v1.pdf)]
- **[Seewoâ€™s Submission to MLC-SLM: Lessons learned from Speech Reasoning Language Models (2025)]**  
  Seewo 2025å¹´Interspeech MLC-SLM æŒ‘æˆ˜èµ›è®ºæ–‡,å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹  [[arXiv](https://arxiv.org/pdf/2506.13300v3.pdf)]
- **[Qwen vs. Gemma Integration with Whisper: A Comparative Study in Multilingual SpeechLLM Systems (2025)]**  
  Qwen vs. Gemma é›†æˆ Whisperï¼šå¤šè¯­è¨€è¯­éŸ³LLMç³»ç»Ÿæ¯”è¾ƒç ”ç©¶ [[arXiv](https://arxiv.org/pdf/2506.13596.pdf)]
- **[SHNU Multilingual Conversational Speech Recognition System for INTERSPEECH 2025 MLC-SLM Challenge (2025)]**  
  ä¸Šæµ·å¸ˆèŒƒå¤§å­¦ 2025å¹´Interspeech MLC-SLM æŒ‘æˆ˜èµ›è®ºæ–‡ [[arXiv](https://arxiv.org/pdf/2507.03343.pdf)]
- **[CMT-LLM: Contextual Multi-Talker ASR Utilizing Large Language Models (2025)]**  
  åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å¤šè¯´è¯äººè¯­éŸ³è¯†åˆ« [[arXiv](https://arxiv.org/pdf/2506.12059v1.pdf)]
- **[Whale: Large-Scale multilingual ASR model with w2v-BERT and E-Branchformer with large speech data (2025)]**
  åŸºäºw2v-BERTå’ŒE-Branchformerçš„å¤§å‹å¤šè¯­è¨€ASRæ¨¡å‹ [[arXiv](https://arxiv.org/pdf/2506.01439v1.pdf)]
- **[Improving Multilingual Speech Models on ML-SUPERB 2.0: Fine-tuning with Data Augmentation and LID-Aware CTC (2025)]**  
  æ”¹è¿›ML-SUPERB 2.0ä¸Šçš„å¤šè¯­è¨€è¯­éŸ³æ¨¡å‹ï¼šæ•°æ®å¢å¼ºå’ŒLIDæ„ŸçŸ¥CTCå¾®è°ƒ [[arXiv](https://arxiv.org/pdf/2505.24200v2.pdf)]
- **[Delayed-KD: Delayed Knowledge Distillation based CTC for Low-Latency Streaming ASR (2025)]**  
  å»¶è¿ŸçŸ¥è¯†è’¸é¦çš„ä½å»¶è¿Ÿæµå¼ASR [[arXiv](https://arxiv.org/pdf/2505.22069.pdf)]
- **[GigaAM: Efficient Self-Supervised Learner for Speech Recognition (2025)]**  
  ä¿„è¯­è¯­éŸ³è¯†åˆ«ï¼šé«˜æ•ˆçš„è‡ªç›‘ç£å­¦ä¹ è€…ç”¨äºè¯­éŸ³è¯†åˆ« [[arXiv](https://arxiv.org/pdf/2505.21999v1.pdf)]
- **[Exploring Generative Error Correction for Dysarthric Speech Recognition (2025)]**
  æ¢ç´¢ç”Ÿæˆå¼é”™è¯¯çº æ­£ä»¥æ”¹å–„å‘éŸ³éšœç¢è¯­éŸ³è¯†åˆ« [[arXiv](https://arxiv.org/pdf/2505.20163v1.pdf)]
- **[ILT: Iterative LoRA Training through Focusâ€“Feedbackâ€“Fix for Multilingual Speech Recognition (2025)]**  
  è¿­ä»£LoRAè®­ç»ƒï¼šèšç„¦-åé¦ˆ-ä¿®å¤ç”¨äºå¤šè¯­è¨€è¯­éŸ³è¯†åˆ« [[arXiv](https://arxiv.org/pdf/2507.08477.pdf)]
- **[GigaSpeech 2: An Evolving, Large-Scale and Multi-domain ASR Corpus for Low-Resource Languages with Automated Crawling, Transcription and Refinement (2025)]**  
  2025å¹´GigaSpeech 2ï¼šç”¨äºä½èµ„æºè¯­è¨€çš„è‡ªåŠ¨çˆ¬å–ã€è½¬å½•å’Œæ”¹è¿›çš„å¤§å‹å¤šé¢†åŸŸASRè¯­æ–™åº“ [[arXiv](https://arxiv.org/pdf/2406.11546.pdf)]
- **[Enabling Auditory Large Language Models for Automatic Speech Quality Evaluation (2025)]**  
  ç”¨äºè‡ªåŠ¨è¯­éŸ³è´¨é‡è¯„ä¼°çš„å¬è§‰å¤§å‹è¯­è¨€æ¨¡å‹ [[arXiv](https://arxiv.org/pdf/2505.19967v1.pdf)]
- **[Performance Evaluation of SLAM-ASR: The Good, the Bad, the Ugly, and the Way Forward (2025)]**  
  SLAM-ASRçš„æ€§èƒ½è¯„ä¼°ï¼šå¥½ã€åã€ä¸‘é™‹å’Œå‰è¿›æ–¹å‘ [[arXiv](https://arxiv.org/pdf/2411.03866.pdf)]
- **[Speech Prefix-Tuning with RNNT Loss for Improving LLM Predictions (2025)]**  
  ä½¿ç”¨RNNTæŸå¤±è¿›è¡Œè¯­éŸ³å‰ç¼€è°ƒä¼˜ä»¥æé«˜LLMé¢„æµ‹ [[arXiv](https://arxiv.org/pdf/2406.14701.pdf)]
- **[Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation (2025)]**  
  å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘çš„ä½ç§©å’Œç¨€ç–æ¨¡å‹åˆå¹¶ [[arXiv](https://arxiv.org/pdf/2505.19893v1.pdf)]
- **[Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation (2025)]**  
  ç”¨äºè¯­éŸ³ç”Ÿæˆçš„Emiliaï¼šå¤§è§„æ¨¡ã€å¹¿æ³›ã€å¤šè¯­è¨€å’Œå¤šæ ·åŒ–çš„æ•°æ®é›† [[arXiv](https://arxiv.org/pdf/2501.15907.pdf)]
- **[Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR (2025)]**  
  ç”¨äºå¤šè¯­è¨€ASRä¸­å‚æ•°é«˜æ•ˆæ— ç¼è¯­è¨€æ‰©å±•çš„è¯­è¨€æ„ŸçŸ¥æç¤ºè°ƒä¼˜ [[arXiv](https://arxiv.org/pdf/2506.21577.pdf)]
- **[Efficient Multilingual ASR Finetuning via LoRA Language Experts (2025)]**  
  é€šè¿‡LoRAè¯­è¨€ä¸“å®¶è¿›è¡Œé«˜æ•ˆå¤šè¯­è¨€ASRå¾®è°ƒ [[arXiv](https://arxiv.org/pdf/2506.21555.pdf)]
- **[OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and Cleaning (2025)]**  
  é€šè¿‡æ•°æ®ç¼©æ”¾å’Œæ¸…ç†æ”¹è¿›å¼€æ”¾å¼Whisperé£æ ¼è¯­éŸ³æ¨¡å‹ [[arXiv](https://arxiv.org/pdf/2506.00338.pdf)]
- **[Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs (2025)]**  
  é€šè¿‡æ··åˆLoRAè¿›è¡Œç´§å‡‘è€Œå¼ºå¤§çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ [[arXiv](https://arxiv.org/pdf/2503.01743.pdf)]
- **[Voxtral (2025)]**  
  Voxtral æ¨¡å‹ [[arXiv](https://arxiv.org/pdf/2507.13264.pdf)]
- **[Granite-speech: open-source speech-aware LLMs with strong English ASR capabilities (2025)]**  
  å…·æœ‰å¼ºå¤§è‹±è¯­ASRèƒ½åŠ›çš„å¼€æºè¯­éŸ³æ„ŸçŸ¥LLMs [[arXiv](https://arxiv.org/pdf/2505.08699.pdf)]
- **[: The First Large-Scale Open-Science Speech Foundation Model for English and Italian (2025)]**  
  è‹±è¯­å’Œæ„å¤§åˆ©è¯­çš„ç¬¬ä¸€å¤§æ•°æ®é›†å¼€æºè¯­éŸ³åŸºç¡€æ¨¡å‹ [[arXiv](https://arxiv.org/pdf/2505.22759.pdf)]
- **[From Tens of Hours to Tens of Thousands: Scaling Back-Translation for Speech Recognition (2025)]**  
  ä»æ•°åå°æ—¶åˆ°æ•°ä¸‡å°æ—¶ï¼šé€šè¿‡å›è¯‘æ‰©å±•è¯­éŸ³è¯†åˆ« [[arXiv](https://arxiv.org/pdf/2505.16972.pdf)]
- **[The TEA-ASLP System for Multilingual Conversational Speech Recognition and Speech Diarization in MLC-SLM 2025 Challenge (2025)]**  
  MLC-SLM 2025æŒ‘æˆ˜ä¸­çš„å¤šè¯­è¨€å¯¹è¯è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³åˆ†ç¦»ç³»ç»Ÿ [[arXiv](https://arxiv.org/pdf/2507.18051.pdf)]
- **[SpecASR: Accelerating LLM-based Automatic Speech Recognition via Speculative Decoding (2025)]**  
  é€šè¿‡æ¨æµ‹è§£ç åŠ é€ŸåŸºäºLLMçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ« [[arXiv](https://arxiv.org/pdf/2507.18181.pdf)]
- **[An approach to measuring the performance of Automatic Speech Recognition(ASR) models in the context of Large Language Model(LLM) powered applications (2025)]**  
  åœ¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„ä¸Šä¸‹æ–‡ä¸­è¡¡é‡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹çš„æ€§èƒ½çš„æ–¹æ³• [[arXiv](https://arxiv.org/pdf/2507.16456.pdf)]
- **[Code-Switching in End-to-End Automatic Speech Recognition: A Systematic Literature Review (2025)]**  
  ç«¯åˆ°ç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä¸­çš„ä»£ç åˆ‡æ¢ï¼šç³»ç»Ÿæ–‡çŒ®ç»¼è¿° [[arXiv](https://arxiv.org/pdf/2507.07741.pdf)]
- **[The Eloquence team submission for task 1 of MLC-SLM challenge (2025)]**  
  MLC-SLMæŒ‘æˆ˜ä»»åŠ¡1çš„Eloquenceå›¢é˜Ÿæäº¤ [[arXiv](https://arxiv.org/pdf/2507.19308.pdf)]
- **[Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization (2025)]**  
  ä½¿ç”¨åŸºäºLLMçš„ä¸­é—´æŸå¤±æ­£åˆ™åŒ–å¢å¼ºCTC ASR [[arXiv](https://arxiv.org/pdf/2506.22846.pdf)]
- **[Unifying Streaming and Non-streaming Zipformer-based ASR (2025)]**  
  ç»Ÿä¸€æµå¼å’Œéæµå¼Zipformer ASR [[arXiv](https://www.arxiv.org/pdf/2506.14434.pdf)]
- **[Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition (2025)]**  
  åœ¨ç¨€ç–æ··åˆä¸“å®¶ä¸­ç»Ÿä¸€è·¯ç”±å†³ç­–ä»¥è¿›è¡Œè¯­éŸ³è¯†åˆ« [[arXiv](https://arxiv.org/pdf/2507.05724.pdf)]
- **[Granary: Speech Recognition and Translation Dataset in 25 European Languages (2025)]**  
  25ç§æ¬§æ´²è¯­è¨€çš„è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘æ•°æ®é›† [[arXiv](https://arxiv.org/pdf/2505.13404.pdf)]
- **[Switch Conformer with Universal Phonetic Experts for Multilingual ASR (2025)]**  
  ç”¨äºå¤šè¯­è¨€ASRçš„é€šç”¨éŸ³ç´ ä¸“å®¶åˆ‡æ¢Conformer [[arXiv](https://www.isca-archive.org/interspeech_2025/mimura25_interspeech.pdf)]
- **[Improving Generalization of End-to-End ASR through Diversity and Independence Regularization (2025)]**  
  é€šè¿‡å¤šæ ·æ€§å’Œç‹¬ç«‹æ€§æ­£åˆ™åŒ–æé«˜ç«¯åˆ°ç«¯ASRçš„æ³›åŒ–æ€§ [[arXiv](https://www.isca-archive.org/interspeech_2025/ko25_interspeech.pdf)]
- **[Towards Efficiently Whisper Fine-tuning with Monotonic Alignments (2025)]**  
  é€šè¿‡å•è°ƒå¯¹é½å®ç°é«˜æ•ˆçš„Whisperå¾®è°ƒ [[arXiv](https://www.isca-archive.org/interspeech_2025/zhuang25_interspeech.pdf)]
- **[REB-former: RWKV-enhanced E-branchformer for Speech Recognition (2025)]**  
  åŸºäºRWKVå¢å¼ºçš„E-branchformerç”¨äºè¯­éŸ³è¯†åˆ« [[arXiv](https://www.isca-archive.org/interspeech_2025/song25b_interspeech.pdf)]
- **[AISHELL-5: The First Open-Source In-Car Multi-Channel Multi-Speaker Speech Dataset for Automatic Speech Diarization and Recognition (2025)]**  
  AISHELL-5ï¼šç¬¬ä¸€ä¸ªç”¨äºè‡ªåŠ¨è¯­éŸ³åˆ†ç¦»å’Œè¯†åˆ«çš„å¼€æ”¾å¼è½¦å†…å¤šé€šé“å¤šè¯´è¯äººè¯­éŸ³æ•°æ®é›† [[arXiv](https://www.isca-archive.org/interspeech_2025/dai25c_interspeech.pdf)]
- **[Better Semi-supervised Learning for Multi-domain ASR Through Incremental Retraining and Data Filtering (2025)]**  
  é€šè¿‡å¢é‡å†è®­ç»ƒå’Œæ•°æ®è¿‡æ»¤æé«˜Multi-domain ASRçš„åŠç›‘ç£å­¦ä¹  [[arXiv](https://www.isca-archive.org/interspeech_2025/carofilis25_interspeech.pdf)]
- **[ReazonSpeech: A Free and Massive Corpus for Japanese ASR (2025)]**  
  ReazonSpeechï¼šç”¨äºæ—¥è¯­ASRçš„å…è´¹ä¸”åºå¤§çš„è¯­æ–™åº“ [[arXiv](https://research.reazon.jp/_static/reazonspeech_nlp2023.pdf)]
- **[ASR Error Correction using Large Language Models (2025)]**  
  ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡ŒASRé”™è¯¯çº æ­£ [[arXiv](https://arxiv.org/pdf/2409.09554.pdf)]
- **[Chain-of-Thought Prompting for Speech Translation (2025)]**  
  åŸºäºæ€ç»´é“¾æç¤ºçš„è¯­éŸ³ç¿»è¯‘ [[arXiv](https://arxiv.org/pdf/2409.11538?)]
- **[Investigation of Whisper ASR Hallucinations Induced by Non-Speech Audio (2025)]**  
  éè¯­éŸ³éŸ³é¢‘å¼•èµ·çš„Whisper ASRå¹»è§‰ç ”ç©¶ [[arXiv](https://arxiv.org/pdf/2501.11378?)]
- **[CR-CTC: CONSISTENCY REGULARIZATION ON CTC FOR IMPROVED SPEECH RECOGNITION (2025)]**  
  CTCä¸€è‡´æ€§æ­£åˆ™åŒ–ç”¨äºæ”¹è¿›è¯­éŸ³è¯†åˆ« [[arXiv](https://arxiv.org/pdf/2410.05101)]
- **[WenetSpeech-Yue: A Large-scale Cantonese Speech Corpus with Multi-dimensional Annotation (2025)]**  
  WenetSpeech-Yueï¼šå…·æœ‰å¤šç»´æ³¨é‡Šçš„å¤§è§„æ¨¡ç²¤è¯­è¯­éŸ³è¯­æ–™åº“ [[arXiv](https://arxiv.org/pdf/2509.03959)]
- **[SpeechLLM: Unified Speech and Language Model for Enhanced Multi-Task Understanding in Low Resource Settings (2025)]**  
  SpeechLLMï¼šç”¨äºä½èµ„æºè®¾ç½®ä¸­å¤šä»»åŠ¡ç†è§£çš„ç»Ÿä¸€è¯­éŸ³å’Œè¯­è¨€æ¨¡å‹ [[arXiv](https://arxiv.org/pdf/2509.04473)]
- **[Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling (2025)]**  
  å»¶è¿Ÿæµå»ºæ¨¡çš„æµå¼åºåˆ—åˆ°åºåˆ—å­¦ä¹  [[arXiv](https://arxiv.org/pdf/2509.08753)]
- **[Denoising GER: A Noise-Robust Generative Error Correction with LLM for Speech Recognition (2025)]**  
  åŸºäºLLMçš„å™ªå£°é²æ£’ç”Ÿæˆé”™è¯¯çº æ­£ï¼ˆDenoising GERï¼‰ç”¨äºè¯­éŸ³è¯†åˆ« [[arXiv](https://arxiv.org/pdf/2509.08753)]

</details>

---

<details>
<summary><b>ğŸµ è¯­éŸ³åˆæˆ (TTS)</b></summary>

### å·¥å…·

| åç§° | æè¿° | é“¾æ¥ |
|------|------|------|
| VITS | åŸºäºVAEçš„ç«¯åˆ°ç«¯TTSæ¨¡å‹ | [GitHub](https://github.com/jaywalnut310/vits) |
| Tacotron 2 | Google ç¥ç»TTSæ¶æ„ | [GitHub](https://github.com/NVIDIA/tacotron2) |
| FastSpeech | éè‡ªå›å½’å¿«é€Ÿåˆæˆ | [GitHub](https://github.com/ming024/FastSpeech) |
| wetts | ä¸­æ–‡TTSå·¥å…·åŒ… | [GitHub](https://github.com/wenet-e2e/wetts) |
| index-tts | åŸºäºLLMçš„å·¥ä¸šå¯æ§ä¸­è‹±æ–‡åˆæˆ | [GitHub](https://github.com/index-tts/index-tts) |

### è®ºæ–‡

- **[Deep Voice: Real-time Neural Text-to-Speech (2017)]**  
  å®æ—¶ç¥ç»æ–‡æœ¬åˆ°è¯­éŸ³ [[arXiv](https://arxiv.org/abs/1702.07825v2.pdf)]
- **[Deep Voice 2: Multi-Speaker Neural Text-to-Speech (2017)]**  
  Deep Voice 2 å¤šè¯´è¯äººç¥ç»æ–‡æœ¬åˆ°è¯­éŸ³ [[arXiv](https://arxiv.org/abs/1705.08947v2.pdf)]
- **[TACOTRON: TOWARDS END-TO-END SPEECH SYNTHESIS (2017)]**  
  TACOTRONèµ°å‘ç«¯åˆ°ç«¯è¯­éŸ³åˆæˆ [[arXiv](https://arxiv.org/abs/1703.10135v2.pdf)]
- **[VITS (2021)]**  
  å¯¹æŠ—å­¦ä¹ ç«¯åˆ°ç«¯TTS [[arXiv](https://arxiv.org/abs/2106.06103)]
- **[One TTS Alignment To Rule Them All (2021)]**  
  ä¸€ç§TTSå¯¹é½ [[arXiv](https://arxiv.org/abs/2108.10447v1.pdf)]
- **[ON PROSODY MODELING FOR ASR+TTS BASED VOICE CONVERSION (2021)]**  
  åŸºäºASR+TTSçš„è¯­éŸ³è½¬æ¢çš„éŸµå¾‹å»ºæ¨¡ [[arXiv](https://arxiv.org/abs/2107.09477v1.pdf)]
- **[ISTFTNET: FAST AND LIGHTWEIGHT MEL-SPECTROGRAM VOCODER INCORPORATING INVERSE SHORT-TIME FOURIER TRANSFORM (2022)]**  
  ISTFTNETï¼šå¿«é€Ÿä¸”è½»é‡çº§çš„åŒ…å«é€†çŸ­æ—¶å‚…é‡Œå¶å˜æ¢çš„æ¢…å°”é¢‘è°±æ³¢å½¢åˆæˆå™¨ [[arXiv](https://arxiv.org/abs/2203.02395v1.pdf)]
- **[NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models (2024)]**  
  NaturalSpeech3ï¼šå…·æœ‰å› å­ç¼–è§£ç å™¨å’Œæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬è¯­éŸ³åˆæˆ [[arXiv](https://arxiv.org/pdf/2403.03100.pdf)]
- **[Simple and Controllable Music Generation (2024)]**  
  ç®€å•ä¸”å¯æ§çš„éŸ³ä¹ç”Ÿæˆ [[arXiv](https://arxiv.org/pdf/2306.05284)]
- **[GENERATIVE PRE-TRAINING FOR SPEECH WITH FLOW MATCHING (2024)]**  
  åŸºäºæµåŒ¹é…çš„ç”Ÿæˆé¢„è®­ç»ƒ [[arXiv](https://arxiv.org/pdf/2310.16338.pdf)]
- **[ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching (2025)]**  
  å°ç±³å›¢é˜ŸåŸºäºæµåŒ¹é…çš„é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ [[arXiv](https://arxiv.org/abs/2506.13053v2.pdf)]
- **[Comparative Analysis of Fast and High-Fidelity Neural Vocoders for Low-Latency Streaming Synthesis in Resource-Constrained Environments (2025)]**  
  èµ„æºå—é™ç¯å¢ƒä¸‹çš„ä½å»¶è¿Ÿæµå¼åˆæˆæ¯”è¾ƒåˆ†æ [[arXiv](https://arxiv.org/abs/2506.03554v1.pdf)]
- **[CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens (2024)]**  
  é˜¿é‡Œé€šä¹‰CosyVoice 1ï¼šåŸºäºç›‘ç£è¯­ä¹‰æ ‡è®°çš„å¯æ‰©å±•å¤šè¯­è¨€é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆå™¨ [[arXiv](https://arxiv.org/abs/2407.05407.pdf)]
- **[CosyVoice2: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens (2024)]**  
  é˜¿é‡Œé€šä¹‰CosyVoice 2ï¼šåŸºäºç›‘ç£è¯­ä¹‰æ ‡è®°çš„å¯æ‰©å±•å¤šè¯­è¨€é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆå™¨ [[arXiv](https://arxiv.org/abs/2412.10117.pdf)]
- **[CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and Post-training (2025)]**  
  é˜¿é‡Œé€šä¹‰CosyVoice 3 [[arXiv](https://arxiv.org/abs/2505.17589v2.pdf)]
- **[SlimSpeech: Lightweight and Efficient Text-to-Speech with Slim Rectified Flow (2025)]**  
  è½»é‡çº§é«˜æ•ˆæ–‡æœ¬åˆ°è¯­éŸ³ [[arXiv](https://arxiv.org/abs/2504.07776v2.pdf)]
- **[IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System (2025)]**  
  bç«™ï¼šå·¥ä¸šçº§å¯æ§é«˜æ•ˆé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿ [[arXiv](https://arxiv.org/abs/2502.05512.pdf)]
- **[IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech (2025)]**  
  bç«™ï¼šæƒ…æ„Ÿè¡¨è¾¾å’ŒæŒç»­æ—¶é—´æ§åˆ¶çš„è‡ªåŠ¨å›å½’é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³çªç ´ [[arXiv](https://arxiv.org/abs/2506.21619.pdf)]
- **[Spark-TTS An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens (2025)]**  
  Spark-TTS åŸºäºå•æµè§£è€¦è¯­éŸ³æ ‡è®°çš„LLMæ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹ [[arXiv](https://arxiv.org/abs/2503.01710v1.pdf)]
- **[ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching (2025)]**  
  ZipVoice-Dialog åŸºäºæµåŒ¹é…çš„éè‡ªå›å½’å¯¹è¯ç”Ÿæˆ [[arXiv](https://arxiv.org/pdf/2507.09318.pdf)]
- **[MoonCast: High-Quality Zero-Shot Podcast Generation (2025)]**  
  MoonCast é«˜è´¨é‡é›¶æ ·æœ¬æ’­å®¢ç”Ÿæˆ [[arXiv](https://arxiv.org/pdf/2503.14345.pdf)]

</details>

---

<details>
<summary><b>ğŸ”‰ è¯­éŸ³å¢å¼º</b></summary>

### å·¥å…·

| åç§° | æè¿° | é“¾æ¥ |
|------|------|------|
| Demucs | è¯­éŸ³/éŸ³ä¹åˆ†ç¦»å·¥å…· | [GitHub](https://github.com/facebookresearch/demucs) |
| RNNoise | å®æ—¶å™ªå£°æŠ‘åˆ¶ | [GitHub](https://github.com/xiph/rnnoise) |

### è®ºæ–‡

- **[SEGAN (2017)]**  
  é¦–ä¸ªåŸºäºGANçš„è¯­éŸ³å¢å¼º [[arXiv](https://arxiv.org/abs/1703.09452)]

</details>

---

<details>
<summary><b>ğŸ¤– LLMåŠå¤šæ¨¡æ€LLM</b></summary>

### å·¥å…·

| åç§° | æè¿° | é“¾æ¥ |
|------|------|------|
| SpeechGPT | æ”¯æŒè¯­éŸ³äº¤äº’çš„LLM | [GitHub](https://github.com/0nutation/SpeechGPT) |
| Step-Audio | è¯­éŸ³å¯¹è¯å¤§æ¨¡å‹ | [GitHub](https://github.com/stepfun-ai/Step-Audio) |
| LLaMA-Factory | LLM sft tool | [GitHub](https://github.com/hiyouga/LLaMA-Factory) |
| Megatron-LM | Megatron-LM | [GitHub](https://github.com/NVIDIA/Megatron-LM.git) |
| verl | LLMå¼ºåŒ–å­¦ä¹ å·¥å…· | [GitHub](https://github.com/volcengine/verl.git) |
| VeOmni | LLMè®­ç»ƒå·¥å…· | [GitHub](https://github.com/ByteDance-Seed/VeOmni) |

### è®ºæ–‡

- **[LLaMA: Open and Efficient Foundation Language Models (2023)]**
  LLaMAï¼šå¼€æºé«˜æ•ˆçš„åŸºç¡€è¯­è¨€æ¨¡å‹ [[arXiv](https://arxiv.org/abs/2302.13971)]
- **[Qwen2-Audio Technical Report (2024)]**  
  é˜¿é‡Œäº‘Qwen2-AudioæŠ€æœ¯æŠ¥å‘Š [[arXiv](https://arxiv.org/abs/2407.10759)]
- **[Gemma 2: Improving Open Language Models at a Practical Size (2024)]**  
  Gemma 2æŠ€æœ¯æŠ¥å‘Š [[arXiv](https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf)]
- **[Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts (2024)]**
  Uni-MoEï¼šé€šè¿‡ä¸“å®¶æ··åˆå®ç°ç»Ÿä¸€å¤šæ¨¡æ€LLMçš„æ‰©å±• [[arXiv](https://arxiv.org/pdf/2405.11273?)]
- **[Making LLMs Better Many-to-Many Speech-to-Text Translators with Curriculum Learning (2024)]**  
  é€šè¿‡è¯¾ç¨‹å­¦ä¹ ä½¿LLMsæˆä¸ºæ›´å¥½çš„å¤šå¯¹å¤šè¯­éŸ³åˆ°æ–‡æœ¬ç¿»è¯‘è€… [[arXiv](https://arxiv.org/pdf/2409.19510)]
- **[GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities (2023)]**  
  GAMAï¼šå…·æœ‰é«˜çº§éŸ³é¢‘ç†è§£å’Œå¤æ‚æ¨ç†èƒ½åŠ›çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ [[arXiv](https://arxiv.org/pdf/2406.11768)]
- **[Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities (2024)]**  
  Audio Flamingoï¼šå…·æœ‰å°‘æ ·æœ¬å­¦ä¹ å’Œå¯¹è¯èƒ½åŠ›çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ [[arXiv](https://arxiv.org/pdf/2402.01831)]
- **[Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities (2025)]**  
  Audio Flamingo 2ï¼šå…·æœ‰é•¿éŸ³é¢‘ç†è§£å’Œä¸“å®¶æ¨ç†èƒ½åŠ›çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ [[arXiv](https://arxiv.org/pdf/2503.03983)]
- **[Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio Language Models (2025)]**  
  Audio Flamingo 3ï¼šæ¨è¿›éŸ³é¢‘æ™ºèƒ½ [[arXiv](https://arxiv.org/pdf/2507.08128)]
- **[Step-Audio (2025)]**  
  Step-Audio Team è¯­éŸ³å¯¹è¯ [[arXiv](https://arxiv.org/abs/2502.11946)]
- **[Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice (2025)]**  
  Seed LiveInterpret 2.0ï¼šç«¯åˆ°ç«¯åŒæ—¶è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘ [[arXiv](https://arxiv.org/abs/2507.17527)]
- **[Gemma 3 Technical Report (2025)]**  
  Gemma 3æŠ€æœ¯æŠ¥å‘Š [[Gemma3Report](https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf)]
- **[Large Language Models: A Survey (2025)]**  
  å¤§å‹è¯­è¨€æ¨¡å‹ç»¼è¿° [[arXiv](https://arxiv.org/pdf/2402.06196)]
- **[Recent Advances in Speech Language Models: A Survey (2025)]**  
  è¯­éŸ³è¯­è¨€æ¨¡å‹æœ€æ–°è¿›å±•ç»¼è¿° [[arXiv](https://arxiv.org/pdf/2410.03751)]
- **[Step-Audio 2 Technical Report (2025)]**  
  Step-Audio 2æŠ€æœ¯æŠ¥å‘Š [[arXiv](https://arxiv.org/pdf/2507.16632)]  
- **[Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters (2025)]**  
  Seed-Xï¼šæ„å»º7Bå‚æ•°çš„å¼ºå¤§å¤šè¯­è¨€ç¿»è¯‘LLM [[arXiv](https://arxiv.org/pdf/2507.13618)]
- **[MiDashengLM: Efficient Audio Understanding with General Audio Captions (2025)]**  
  MiDashengLMï¼šåŸºäºé€šç”¨éŸ³é¢‘å­—å¹•çš„éŸ³é¢‘ç†è§£ [[arXiv](https://arxiv.org/pdf/2508.03983)]
- **[VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo (2025)]**  
  VeOmni [[arXiv](https://arxiv.org/pdf/2508.02317)]
- **[VIBEVOICE Technical Report (2025)]**  
  VIBEVOICEæŠ€æœ¯æŠ¥å‘Š [[arXiv](https://arxiv.org/pdf/2508.19205)]
- **[SEAL: Speech Embedding Alignment Learning for Speech Large Language Model with Retrieval-Augmented Generation (2025)]**  
  SEALï¼šåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„è¯­éŸ³è¯­è¨€æ¨¡å‹ [[arXiv](https://arxiv.org/pdf/2502.02603)]
- **[LLaMA-Omni 2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis (2025)]**  
  LLaMA-Omni 2ï¼šåŸºäºLLMçš„å®æ—¶è¯­éŸ³èŠå¤©æœºå™¨äºº [[arXiv](https://arxiv.org/pdf/2505.02625?)]
- **[Qwen2.5-Omni Technical Report (2025)]**  
  Qwen2.5-OmniæŠ€æœ¯æŠ¥å‘Š [[arXiv](https://arxiv.org/pdf/2503.20215)]
- **[ESPnet-SpeechLM: An Open Speech Language Model Toolkit (2025)]**  
  ESPnet-SpeechLMï¼šå¼€æºè¯­éŸ³è¯­è¨€æ¨¡å‹å·¥å…·åŒ… [[arXiv](https://arxiv.org/pdf/2502.15218)]

</details>

---

<details>
<summary><b>ğŸ†” å£°çº¹è¯†åˆ« (Speaker Recognition)</b></summary>

### å·¥å…·

| åç§° | æè¿° | é“¾æ¥ |
|------|------|------|
| Resemblyzer | åŸºäºç¥ç»ç½‘ç»œçš„å£°çº¹ç‰¹å¾æå– | [GitHub](https://github.com/resemble-ai/Resemblyzer) |
| PyAnnote | è¯´è¯äººæ—¥å¿—åˆ†æå·¥å…·åŒ… | [GitHub](https://github.com/pyannote/pyannote-audio) |
| ECAPA-TDNN | å½“å‰æœ€ä¼˜å£°çº¹æ¨¡å‹å®ç° | [GitHub](https://github.com/TaoRuijie/ECAPA-TDNN) |
| 3D-Speaker | é˜¿é‡Œçš„å£°çº¹å·¥å…·åŒ… | [GitHub](https://github.com/modelscope/3D-Speaker) |

### è®ºæ–‡

- **[ECAPA-TDNN (2020)]**  
  é€šé“æ³¨æ„åŠ›æœºåˆ¶æ”¹è¿›çš„å£°çº¹æ¨¡å‹ [[arXiv](https://arxiv.org/abs/2005.07143)]
- **[GE2E (2018)]**  
  è°·æ­Œç«¯åˆ°ç«¯å£°çº¹è¯†åˆ« [[arXiv](https://arxiv.org/abs/1710.10467)]

</details>

---

<details>
<summary><b>â° è¯­éŸ³å”¤é†’ (Wake Word Detection)</b></summary>

### å·¥å…·

| åç§° | æè¿° | é“¾æ¥ |
|------|------|------|
| Porcupine | ç¦»çº¿å”¤é†’è¯å¼•æ“ï¼ˆæ”¯æŒè‡ªå®šä¹‰çƒ­è¯ï¼‰ | [GitHub](https://github.com/Picovoice/porcupine) |
| Snowboy | è½»é‡çº§å”¤é†’è¯æ£€æµ‹ï¼ˆå·²å½’æ¡£ï¼‰ | [GitHub](https://github.com/Kitt-AI/snowboy) |
| HeyFriender | å¼€æºå¤šè¯­è¨€å”¤é†’è¯è®­ç»ƒæ¡†æ¶ | [GitHub](https://github.com/HeyFriender/heyfriender) |

### è®ºæ–‡

- **[Keyword Transformer (2021)]**  
  åŸºäºTransformerçš„å”¤é†’è¯æ£€æµ‹ [[arXiv](https://arxiv.org/abs/2104.00769)]
- **[MatchboxNet (2020)]**  
  ç«¯åˆ°ç«¯ä½å»¶è¿Ÿå”¤é†’æ¨¡å‹ [[arXiv](https://arxiv.org/abs/2004.03706)]

</details>

---

## è´¡çŒ®

æ¬¢è¿æäº¤ Pull Request è¡¥å……èµ„æºï¼  
âš ï¸ è¦æ±‚ï¼š  

- æŒ‰åˆ†ç±»æ·»åŠ   
- æä¾›å®˜æ–¹é“¾æ¥  
- è®ºæ–‡éœ€é™„arXiv/DOIé“¾æ¥
